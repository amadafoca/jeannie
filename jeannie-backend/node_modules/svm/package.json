{
  "_args": [
    [
      "svm@*",
      "D:\\Users\\ACCER\\tmp\\jeannie\\node_modules\\limdu"
    ]
  ],
  "_from": "svm@*",
  "_id": "svm@0.1.1",
  "_inCache": true,
  "_installable": true,
  "_location": "/svm",
  "_npmUser": {
    "email": "andrej.karpathy@gmail.com",
    "name": "karpathy"
  },
  "_npmVersion": "1.1.65",
  "_phantomChildren": {},
  "_requested": {
    "name": "svm",
    "raw": "svm@*",
    "rawSpec": "*",
    "scope": null,
    "spec": "*",
    "type": "range"
  },
  "_requiredBy": [
    "/limdu"
  ],
  "_resolved": "https://registry.npmjs.org/svm/-/svm-0.1.1.tgz",
  "_shasum": "3ac1ef565bf641d8edfe4f1606a8a9457a4ab0e4",
  "_shrinkwrap": null,
  "_spec": "svm@*",
  "_where": "D:\\Users\\ACCER\\tmp\\jeannie\\node_modules\\limdu",
  "author": {
    "email": "andrej.karpathy@gmail.com",
    "name": "Andrej Karpathy"
  },
  "bugs": {
    "url": "https://github.com/karpathy/svmjs/issues"
  },
  "dependencies": {},
  "description": "Support Vector Machines",
  "devDependencies": {},
  "directories": {},
  "dist": {
    "shasum": "3ac1ef565bf641d8edfe4f1606a8a9457a4ab0e4",
    "tarball": "https://registry.npmjs.org/svm/-/svm-0.1.1.tgz"
  },
  "homepage": "https://github.com/karpathy/svmjs#readme",
  "keywords": [
    "support vector machines",
    "machine learning",
    "classifier",
    "svm"
  ],
  "main": "./lib/svm",
  "maintainers": [
    {
      "email": "andrej.karpathy@gmail.com",
      "name": "karpathy"
    }
  ],
  "name": "svm",
  "optionalDependencies": {},
  "readme": "# svmjs\nAndrej Karpathy\nJuly 2012\n\nsvmjs is a lightweight implementation of the SMO algorithm to train a binary\nSupport Vector Machine. As this uses the dual formulation, it also supports\narbitrary kernels. Correctness test, together with MATLAB reference code\nare in /test.\n\n## Online GUI demo\n\nCan be found here: http://cs.stanford.edu/~karpathy/svmjs/demo/\n\nCorresponding code is inside /demo directory.\n\n## Usage\n\nThe simplest use case:\n```javascript\n// include the library\n<script src=\"./svmjs/lib/svm.js\"></script>\n<script>\ndata = [[0,0], [0,1], [1,0], [1,1]];\nlabels = [-1, 1, 1, -1];\nsvm = new svmjs.SVM();\nsvm.train(data, labels, {C: 1.0}); // C is a parameter to SVM\ntestlabels = svm.predict(testdata);\n</script>\n```\nHere, `data` and `testdata` are a 2D, NxD array of floats, `labels` and `testlabels`\nis an array of size N that contains 1 or -1. You can also query for the raw margins:\n```javascript\nmargins = svm.margins(testdata);\nmargin = svm.marginOne(testadata[0]);\n```\n\nThe library supports arbitrary kernels, but currently comes with linear and rbf kernel:\n```javascript\nsvm.train(data, labels, { kernel: function(v1,v2){ /* return K(v1, v2) */} }); // arbitrary function\nsvm.train(data, labels, { kernel: 'linear' });\nsvm.train(data, labels, { kernel: 'rbf', rbfsigma: 0.5 }); // sigma in the gaussian kernel = 0.5\n```\n\nFor training you can pass in several options. Here are the defaults:\n```javascript\nvar options = {};\n/* For C, Higher = you trust your data more. Lower = more regularization.\nShould be in range of around 1e-2 ... 1e5 at most. */\noptions.C = 1.0;\noptions.tol = 1e-4; // do not touch this unless you're pro\noptions.alphatol = 1e-7; // used for pruning non-support vectors. do not touch unless you're pro\noptions.maxiter = 10000; // if you have a larger problem, you may need to increase this\noptions.kernel = svmjs.linearKernel; // discussed above\noptions.numpasses = 10; // increase this for higher precision of the result. (but slower)\nsvm.train(data, labels, options);\n```\n\nRules of thumb: You almost always want to try the linear SVM first and see how that works. You want\nto play around with different values of C from about 1e-2 to 1e5, as every dataset is different. `C=1`\nis usually a fairly reasonable value. Roughly, C is the cost to the SVM when it mis-classifies one of your\ntraining examples. If you increase it, the SVM will try very hard to fit all your data, which may be good\nif you strongly trust your data. In practice, you usually don't want it too high though. If linear kernel \ndoesn't work very well, try the rbf kernel. You will have to try different values of both C and just as crucially the sigma for the gaussian kernel. \n\nThe linear SVM should be much faster than SVM with any other kernel. If you want it even faster \nbut less accurate, you want to play around with options.tol (try increase a bit). You can also try to\ndecrease options.maxiter and especially options.numpasses (decrease a bit). \nIf you use non-linear svm, you can also speed up the svm at test by playing around with \noptions.alphatol (try increase a bit).\n\nIf you use linear or rbf kernel (instead of some custom one) you can load and save the svm:\n```javascript\nvar svm = new svmjs.SVM();\nvar json = svm.toJSON();\nvar svm2 = new svmjs.SVM();\nsvm2.fromJSON(json);\n```\n\n## Using in node\nTo use this library in [node.js](http://nodejs.org/), install with `npm`:\n\n```\nnpm install svm\n```\n\nAnd use like so:\n\n```javascript\nvar svm = require(\"svm\");\nvar SVM = new svm.SVM();\nSVM.train(data, labels);\n```\n\n## Implementation details\nThe SMO algorithm is very space efficient, so you need not worry about\nrunning out of space no matter how large your problem is. However, you do need to\nworry about runtime efficiency. In practice, there are many heuristics one can\nuse to select the pair of alphas (i,j) to optimize and this uses a rather naive\napproach. If you have a large and complex problem, you will need to increase\nmaxiter a lot. (or don't use Javascript!)\n\n## License\nMIT\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+ssh://git@github.com/karpathy/svmjs.git"
  },
  "version": "0.1.1"
}
